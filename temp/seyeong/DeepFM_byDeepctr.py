# -*- coding: utf-8 -*-
"""DeepFM_byDeepctr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1reESewr_j9KDhTI_ky16ikV61rWgeANf
"""

pip install deepctr

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')

# %cd /content/drive/My Drive/Tobigs/컨퍼런스_와인추천/

import pandas as pd
import numpy as np
from deepctr.models import DeepFM
from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
import tensorflow as tf
from keras.callbacks import EarlyStopping
from sklearn.metrics import accuracy_score, roc_auc_score

train_df = pd.read_json('Data_최종본/train_all_meta_v2.json')
test_df = pd.read_json('Data_최종본/test_all_meta_v2.json')



"""feature : userID, food, grapes, region_id + 연속형 변수 +wine_id


"""

X_train = train_df.loc[:,['rating_count','rating_average','body','acidity','alcohol','winery_ratings_count','winery_ratings_average','user_follower_count','user_following_count','user_rating_count','userID','wine_id','food','grapes','region_id']]
y_train = train_df['like']
X_test = test_df.loc[:,['rating_count','rating_average','body','acidity','alcohol','winery_ratings_count','winery_ratings_average','user_follower_count','user_following_count','user_rating_count','userID','wine_id','food','grapes','region_id']]
y_test = test_df['like']

# food 리스트 -> str
X_train['food'] = X_train['food'].fillna(False)  # nan -> False
food_str = []
for i in range(len(X_train['food'])):
    if X_train['food'][i]:food_str.append(' '.join(X_train['food'][i]))
    else:food_str.append(' ')
X_train.food = pd.Series(food_str, name='food')

X_test['food'] = X_test['food'].fillna(False)   # nan -> False
food_str = []
for i in range(len(X_test['food'])):
    if X_test['food'][i]:food_str.append(' '.join(X_test['food'][i]))
    else:food_str.append(' ')
X_test.food = pd.Series(food_str, name='food')

# grapes 리스트 -> str
X_train['grapes'] = X_train['grapes'].fillna(False)   # nan -> False
grapes_str = []
for i in range(len(X_train['grapes'])):
    if X_train['grapes'][i]:grapes_str.append(' '.join(X_train['grapes'][i]))
    else:grapes_str.append(' ')
X_train.grapes = pd.Series(grapes_str, name='grapes')

X_test['grapes'] = X_test['grapes'].fillna(False)   # nan -> False
grapes_str = []
for i in range(len(X_test['grapes'])):
    if X_test['grapes'][i]:grapes_str.append(' '.join(X_test['grapes'][i]))
    else:grapes_str.append(' ')
X_test.grapes = pd.Series(grapes_str, name='grapes')

# region_id -> int
X_train.region_id.fillna(0, inplace=True)
X_test.region_id.fillna(0, inplace=True)
X_train.region_id = X_train.region_id.astype(int)
X_test.region_id = X_test.region_id.astype(int)

sparse_features = ['userID', 'wine_id', 'food', 'grapes', 'region_id']

dense_features = ['rating_count', 'rating_average', 'body', 'acidity', 'alcohol',
       'winery_ratings_count', 'winery_ratings_average', 'user_follower_count',
       'user_following_count', 'user_rating_count']

for feat in sparse_features:
    lbe = LabelEncoder()
    all = pd.concat([X_train[feat], X_test[feat]], axis=0).drop_duplicates() # train, test 전체를 묶어서 LabelEncoder fit
    lbe = lbe.fit(all)
    X_train[feat] = lbe.transform(X_train[feat])
    X_test[feat] = lbe.transform(X_test[feat])
    if feat == 'wine_id':wine_id_lbe = lbe


mms = MinMaxScaler(feature_range=(0, 1))
X_train[dense_features] = mms.fit_transform(X_train[dense_features])
X_test[dense_features] = mms.transform(X_test[dense_features])

fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=pd.concat([X_train[feat], X_test[feat]], axis=0).drop_duplicates().nunique(),embedding_dim=4)  # vocabulary_size를 train, test 전체로부터
                           for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,) for feat in dense_features]

dnn_feature_columns = fixlen_feature_columns
linear_feature_columns = fixlen_feature_columns

feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)

# 결측치 mean으로 처리
X_train.fillna(X_train.mean(), inplace=True)
X_test.fillna(X_test.mean(), inplace=True)

train_model_input = {name:X_train[name] for name in feature_names}
test_model_input = {name:X_test[name] for name in feature_names}

# parameters
BATCH_SIZE = 256
EPOCHS = 10
# early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)

model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary')
model.compile("adam", "binary_crossentropy",metrics=['binary_crossentropy'])  # learning rate는 default 0.001

history = model.fit(train_model_input, y_train.values,
                    batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=2, validation_split=0.2)
y_pred = model.predict(test_model_input, batch_size=BATCH_SIZE)
print("test AUC", round(roc_auc_score(y_test.values, y_pred), 4))
print("\ntest Auccuracy", round(accuracy_score(y_test.values, y_pred.round()), 4))

model.summary()




"""feature : userID, wine_id, food, grapes, region_id, country_code, 연속형 변수"""

X_train = train_df.loc[:,['rating_count','rating_average','body','acidity','alcohol','winery_id','userID','wine_id','food','grapes','region_id','country_code']]
y_train = train_df['like']
X_test = test_df.loc[:,['rating_count','rating_average','body','acidity','alcohol','winery_id','userID','wine_id','food','grapes','region_id','country_code']]
y_test = test_df['like']

# food 리스트 -> str
food_str = []
for i in range(len(X_train['food'])):
    if X_train['food'][i]:food_str.append(' '.join(X_train['food'][i]))
    else:food_str.append(' ')
X_train.food = pd.Series(food_str, name='food')

food_str = []
for i in range(len(X_test['food'])):
    if X_test['food'][i]:food_str.append(' '.join(X_test['food'][i]))
    else:food_str.append(' ')
X_test.food = pd.Series(food_str, name='food')

# grapes 리스트 -> str
grapes_str = []
for i in range(len(X_train['grapes'])):
    if X_train['grapes'][i]:grapes_str.append(' '.join(X_train['grapes'][i]))
    else:grapes_str.append(' ')
X_train.grapes = pd.Series(grapes_str, name='grapes')

grapes_str = []
for i in range(len(X_test['grapes'])):
    if X_test['grapes'][i]:grapes_str.append(' '.join(X_test['grapes'][i]))
    else:grapes_str.append(' ')
X_test.grapes = pd.Series(grapes_str, name='grapes')

# region_id -> int
X_train.region_id.fillna(0, inplace=True)
X_test.region_id.fillna(0, inplace=True)
X_train.region_id = X_train.region_id.astype(int)
X_test.region_id = X_test.region_id.astype(int)

# winery_id -> int
X_train.winery_id.fillna(0, inplace=True)
X_test.winery_id.fillna(0, inplace=True)
X_train.winery_id = X_train.winery_id.astype(int)
X_test.winery_id = X_test.winery_id.astype(int)

# country_code
X_train.country_code.fillna('un', inplace=True)
X_test.country_code.fillna('un', inplace=True)

sparse_features = ['userID', 'wine_id', 'food', 'grapes', 'region_id', 'country_code', 'winery_id']

dense_features = ['rating_count', 'rating_average', 'body', 'acidity', 'alcohol']

for feat in sparse_features:
    lbe = LabelEncoder()
    all = pd.concat([X_train[feat], X_test[feat]], axis=0).drop_duplicates() # train, test 전체를 묶어서 LabelEncoder fit
    lbe = lbe.fit(all)
    X_train[feat] = lbe.transform(X_train[feat])
    X_test[feat] = lbe.transform(X_test[feat])
    if feat == 'wine_id':wine_id_lbe = lbe


mms = MinMaxScaler(feature_range=(0, 1))
X_train[dense_features] = mms.fit_transform(X_train[dense_features])
X_test[dense_features] = mms.transform(X_test[dense_features])

fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=pd.concat([X_train[feat], X_test[feat]], axis=0).drop_duplicates().nunique(),embedding_dim=EMBEDDING_DIM)  # vocabulary_size를 train, test 전체로부터
                           for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,) for feat in dense_features]

dnn_feature_columns = fixlen_feature_columns
linear_feature_columns = fixlen_feature_columns

feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)

# 결측치 mean으로 처리
X_train.fillna(X_train.mean(), inplace=True)
X_test.fillna(X_test.mean(), inplace=True)

train_model_input = {name:X_train[name] for name in feature_names}
test_model_input = {name:X_test[name] for name in feature_names}

# parameters
BATCH_SIZE = 256
EPOCHS = 100
EMBEDDING_DIM = 20
DROPOUT_RATE = 0.25
early_stopping = EarlyStopping(monitor='val_loss', verbose=1, patience=10)

model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary', dnn_dropout=DROPOUT_RATE, dnn_use_bn=True, l2_reg_embedding=0.01)
model.compile("adam", "binary_crossentropy",metrics=['accuracy'])  # learning rate는 default 0.001

history = model.fit(train_model_input, y_train.values,
                    batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=2, validation_split=0.2, callbacks=[early_stopping])  # 
y_pred = model.predict(test_model_input, batch_size=BATCH_SIZE)
print("test AUC", round(roc_auc_score(y_test.values, y_pred), 4))
print("\ntest Auccuracy", round(accuracy_score(y_test.values, y_pred.round()), 4))

model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary', dnn_dropout=DROPOUT_RATE, dnn_use_bn=True, l2_reg_linear=0.0001, l2_reg_embedding=0.0001)
model.compile("adam", "binary_crossentropy",metrics=['accuracy'])  # learning rate는 default 0.001

history = model.fit(train_model_input, y_train.values,
                    batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=2, validation_split=0.2, callbacks=[early_stopping])  # 
y_pred = model.predict(test_model_input, batch_size=BATCH_SIZE)
print("test AUC", round(roc_auc_score(y_test.values, y_pred), 4))
print("\ntest Auccuracy", round(accuracy_score(y_test.values, y_pred.round()), 4))

model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary', dnn_dropout=DROPOUT_RATE, dnn_use_bn=True)
model.compile("adam", "binary_crossentropy",metrics=['accuracy'])  # learning rate는 default 0.001

history = model.fit(train_model_input, y_train.values,
                    batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=2, validation_split=0.2)  # , callbacks=[early_stopping]
y_pred = model.predict(test_model_input, batch_size=BATCH_SIZE)
print("test AUC", round(roc_auc_score(y_test.values, y_pred), 4))
print("\ntest Auccuracy", round(accuracy_score(y_test.values, y_pred.round()), 4))

model.summary()




"""dropout, batch normalization 안하고 100번 돌릴 경우"""

model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary')
model.compile("adam", "binary_crossentropy",metrics=['accuracy'])  # learning rate는 default 0.001

history = model.fit(train_model_input, y_train.values,
                    batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=2, validation_split=0.2)  # , callbacks=[early_stopping]
y_pred = model.predict(test_model_input, batch_size=BATCH_SIZE)
print("test AUC", round(roc_auc_score(y_test.values, y_pred), 4))
print("\ntest Auccuracy", round(accuracy_score(y_test.values, y_pred.round()), 4))

model.summary()




"""embedding_dim 32로 늘리고 early_stopping train accuracy로 걸어두는 경우 **(dropout & batchnormalization O)**"""

# parameters
BATCH_SIZE = 256
EPOCHS = 500
EMBEDDING_DIM = 32
DROPOUT_RATE = 0.25
early_stopping = EarlyStopping(monitor='accuracy', verbose=1, patience=10)

sparse_features = ['userID', 'wine_id', 'food', 'grapes', 'region_id', 'country_code', 'winery_id']

dense_features = ['rating_count', 'rating_average', 'body', 'acidity', 'alcohol']

for feat in sparse_features:
    lbe = LabelEncoder()
    all = pd.concat([X_train[feat], X_test[feat]], axis=0).drop_duplicates() # train, test 전체를 묶어서 LabelEncoder fit
    lbe = lbe.fit(all)
    X_train[feat] = lbe.transform(X_train[feat])
    X_test[feat] = lbe.transform(X_test[feat])
    if feat == 'wine_id':wine_id_lbe = lbe
    if feat == 'food':food_lbe = lbe
    if feat == 'grapes':grapes_lbe = lbe


mms = MinMaxScaler(feature_range=(0, 1))
X_train[dense_features] = mms.fit_transform(X_train[dense_features])
X_test[dense_features] = mms.transform(X_test[dense_features])

fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=pd.concat([X_train[feat], X_test[feat]], axis=0).drop_duplicates().nunique(),embedding_dim=EMBEDDING_DIM)  # vocabulary_size를 train, test 전체로부터
                           for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,) for feat in dense_features]

dnn_feature_columns = fixlen_feature_columns
linear_feature_columns = fixlen_feature_columns

feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)

# 결측치 mean으로 처리
X_train.fillna(X_train.mean(), inplace=True)
X_test.fillna(X_test.mean(), inplace=True)

train_model_input = {name:X_train[name] for name in feature_names}
test_model_input = {name:X_test[name] for name in feature_names}

model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary', dnn_dropout=DROPOUT_RATE, dnn_use_bn=True)
model.compile("adam", "binary_crossentropy",metrics=['accuracy'])  # learning rate는 default 0.001

history = model.fit(train_model_input, y_train.values,
                    batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=2, validation_split=0.2, callbacks=[early_stopping])  # 
y_pred = model.predict(test_model_input, batch_size=BATCH_SIZE)
print("test AUC", round(roc_auc_score(y_test.values, y_pred), 4))
print("\ntest Auccuracy", round(accuracy_score(y_test.values, y_pred.round()), 4))

model.summary()




"""embedding_dim 32로 늘리고 early_stopping train accuracy로 걸어두는 경우 **(dropout & batchnormalization X)**"""

model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary')
model.compile("adam", "binary_crossentropy",metrics=['accuracy'])  # learning rate는 default 0.001

history = model.fit(train_model_input, y_train.values,
                    batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=2, validation_split=0.2, callbacks=[early_stopping])  # 
y_pred = model.predict(test_model_input, batch_size=BATCH_SIZE)
print("test AUC", round(roc_auc_score(y_test.values, y_pred), 4))
print("\ntest Auccuracy", round(accuracy_score(y_test.values, y_pred.round()), 4))

model.summary()





"""#### 최선의 모델의 embedding으로 새로운 유저에게 추천해보기"""

model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary', dnn_dropout=DROPOUT_RATE, dnn_use_bn=True)
model.compile("adam", "binary_crossentropy",metrics=['accuracy'])  # learning rate는 default 0.001

history = model.fit(train_model_input, y_train.values,
                    batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=0, validation_split=0.2, callbacks=[early_stopping])  # 
y_pred = model.predict(test_model_input, batch_size=BATCH_SIZE)
print("test AUC", round(roc_auc_score(y_test.values, y_pred), 4))
print("\ntest Auccuracy", round(accuracy_score(y_test.values, y_pred.round()), 4))

model.save_weights('DeepFM_20210101.h5')
print("model saved!")

X_train = train_df.loc[:,['rating_count','rating_average','body','acidity','alcohol','winery_id','userID','wine_id','food','grapes','region_id','country_code']]
y_train = train_df['like']
X_test = test_df.loc[:,['rating_count','rating_average','body','acidity','alcohol','winery_id','userID','wine_id','food','grapes','region_id','country_code']]
y_test = test_df['like']

# food 리스트 -> str
food_str = []
for i in range(len(X_train['food'])):
    if X_train['food'][i]:food_str.append('/'.join(X_train['food'][i]))  # 구분자 '/'로 변경
    else:food_str.append('')
X_train.food = pd.Series(food_str, name='food')

food_str = []
for i in range(len(X_test['food'])):
    if X_test['food'][i]:food_str.append('/'.join(X_test['food'][i]))
    else:food_str.append('')
X_test.food = pd.Series(food_str, name='food')

# grapes 리스트 -> str
grapes_str = []
for i in range(len(X_train['grapes'])):
    if X_train['grapes'][i]:grapes_str.append('/'.join(X_train['grapes'][i]))
    else:grapes_str.append('')
X_train.grapes = pd.Series(grapes_str, name='grapes')

grapes_str = []
for i in range(len(X_test['grapes'])):
    if X_test['grapes'][i]:grapes_str.append('/'.join(X_test['grapes'][i]))
    else:grapes_str.append('')
X_test.grapes = pd.Series(grapes_str, name='grapes')

# region_id -> int
X_train.region_id.fillna(0, inplace=True)
X_test.region_id.fillna(0, inplace=True)
X_train.region_id = X_train.region_id.astype(int)
X_test.region_id = X_test.region_id.astype(int)

# winery_id -> int
X_train.winery_id.fillna(0, inplace=True)
X_test.winery_id.fillna(0, inplace=True)
X_train.winery_id = X_train.winery_id.astype(int)
X_test.winery_id = X_test.winery_id.astype(int)

# country_code
X_train.country_code.fillna('un', inplace=True)
X_test.country_code.fillna('un', inplace=True)

EMBEDDING_DIM = 32
DROPOUT_RATE = 0.25

sparse_features = ['userID', 'wine_id', 'food', 'grapes', 'region_id', 'country_code', 'winery_id']

dense_features = ['rating_count', 'rating_average', 'body', 'acidity', 'alcohol']

for feat in sparse_features:
    lbe = LabelEncoder()
    all = pd.concat([X_train[feat], X_test[feat]], axis=0).drop_duplicates() # train, test 전체를 묶어서 LabelEncoder fit
    lbe = lbe.fit(all)
    X_train[feat] = lbe.transform(X_train[feat])
    X_test[feat] = lbe.transform(X_test[feat])
    if feat == 'wine_id':wine_id_lbe = lbe
    if feat == 'food':food_lbe = lbe
    if feat == 'grapes':grapes_lbe = lbe

# for feat in dense_features:
#     mms = MinMaxScaler(feature_range=(0, 1))
#     X_train[feat] = mms.fit_transform(X_train[feat].values.reshape(-1,1))
#     X_test[feat] = mms.transform(X_test[feat].values.reshape(-1,1))
#     if feat == 'body':body_mms = mms
#     if feat == 'acidity':acidity_mms = mms
#     if feat == 'alcohol':alcohol_mms = mms

mms = MinMaxScaler(feature_range=(0, 1))
X_train[dense_features] = mms.fit_transform(X_train[dense_features])
X_test[dense_features] = mms.transform(X_test[dense_features])

fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=pd.concat([X_train[feat], X_test[feat]], axis=0).drop_duplicates().nunique(),embedding_dim=EMBEDDING_DIM)  # vocabulary_size를 train, test 전체로부터
                           for i,feat in enumerate(sparse_features)] + [DenseFeat(feat, 1,) for feat in dense_features]

dnn_feature_columns = fixlen_feature_columns
linear_feature_columns = fixlen_feature_columns
feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)

# 결측치 mean으로 처리
X_train.fillna(X_train.mean(), inplace=True)
X_test.fillna(X_test.mean(), inplace=True)

model_load = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary', dnn_dropout=DROPOUT_RATE, dnn_use_bn=True)
model_load.load_weights('DeepFM_20210101.h5')

# model embedding
def get_embedding_weights(dnn_feature_columns,model):
    embedding_dict = {}
    for fc in dnn_feature_columns:
        if hasattr(fc,'embedding_name'):
            if fc.embedding_name is not None:
                name = fc.embedding_name
            else:
                name = fc.name
            embedding_dict[name] = model.get_layer("sparse_emb_"+name).get_weights()[0]
    return embedding_dict
    
embedding_dict = get_embedding_weights(fixlen_feature_columns, model_load)

wine_id_emb = embedding_dict['wine_id']
food_emb = embedding_dict['food']

food_list_label = pd.concat([train_df.food, X_train.food], axis=1, keys=['list', 'label']).drop_duplicates(['label']).reset_index(drop=True)
food_all = pd.concat([food_list_label, pd.Series(list(food_emb[food_list_label.label]), name='emb')], axis=1)
food_all

wine_id_label = pd.concat([pd.concat([train_df.wine_id, test_df.wine_id], axis=0), pd.concat([X_train.wine_id, X_test.wine_id], axis=0)], axis=1, keys=['wine_id', 'label']).drop_duplicates(['wine_id']).reset_index(drop=True)
wine_id_all = pd.concat([wine_id_label, pd.Series(list(wine_id_emb[wine_id_label.label]), name='emb')], axis=1)
wine_id_all

# recommend_new_input에 추가로 들어가는 parameter : food_all, wine_id_all, wine_id_emb, body_mms, acidity_mms, alcohol_mms, X_train, X_test, wine_id_lbe

def recommend_new_input(model, input_food, input_body, input_acidity, input_alcohol, K):
    
    # food_all에서 input_food와 가장 유사한 것 jaccard similarity로 추출
    test_food = set(input_food)
    all_food = [set(food) for food in food_all.list]
    union = [test_food.union(i) for i in all_food]
    intersection = [test_food.intersection(i) for i in all_food]
    jaccard = [len(inter)/len(uni) for inter,uni in zip(intersection, union)]
    input_food_emb = food_all.emb[np.array(jaccard).argsort()[0]]

    # food embedding을 wine_id embedding과 dot product해서 score 산출
    score_by_food = pd.concat([wine_id_all[['wine_id', 'label']], pd.Series(np.dot(input_food_emb, wine_id_emb[wine_id_all.label,:].T), name='score_by_food')], axis=1)

    # body, acidity, alcohol은 모든 wine의 body, acidity, alcohol과 dot product해서 score 산출
    input_body = (input_body-train_df.body.min())/(train_df.body.max()-train_df.body.min())
    input_acidity = (input_acidity-train_df.acidity.min())/(train_df.acidity.max()-train_df.acidity.min())
    input_alcohol = (input_alcohol-train_df.alcohol.min())/(train_df.alcohol.max()-train_df.alcohol.min())
    input_baa = [input_body, input_acidity, input_alcohol]

    all_baa = pd.concat([X_train, X_test], axis=0)[['wine_id', 'body', 'acidity', 'alcohol']].drop_duplicates('wine_id')
    all_baa.drop(['wine_id'], axis=1, inplace=True)
    all_baa = np.array(all_baa)
    score_by_baa = pd.Series(np.dot(input_baa, all_baa.T), name='score_by_baa')

    score_all = pd.concat([score_by_food, score_by_baa], axis=1)
    score_all['final'] = score_all.score_by_food + score_by_baa
    score_all = score_all.sort_values(['final'], ascending=False)[:K]

    return score_all

food_list_all = []
for food in food_list:food_list_all.extend(food)
food_list_all = pd.Series(food_list_all).value_counts().index[:-1]

print(food_list_all)
input_food = input('다음 중 와인과 함께 곁들여 먹을 음식을 입력하세요 : ').split(',')
input_body = int(input('원하는 바디감을 1,2,3,4,5 중 하나로 입력하세요 : '))
input_acidity = int(input('원하는 산도를 1,2,3 중 하나로 입력하세요 : '))
input_alcohol = int(input('원하는 도수를 5,7,9,11,13 중 하나로 입력하세요 : '))

recommend_new_input(model_load, input_food, input_body, input_acidity, input_alcohol, 10)





"""Hit rate"""

test_all = pd.concat([X_test, y_test], axis=1)
test_all = test_all[test_all.like==1]
hit_user_meta = test_all[['userID','user_follower_count','user_following_count','user_rating_count']]
hit_wine_true = test_all['wine_id']

wine_meta = pd.concat([X_train, X_test], axis=0)[['wine_id','rating_count','rating_average','body','acidity','alcohol','winery_ratings_count','winery_ratings_average','food','grapes','region_id']].drop_duplicates(['wine_id']).reset_index(drop=True)

K = 5000
i, hit = 0, 0
wine_id = pd.Series(wine_id_lbe.inverse_transform(wine_meta.wine_id), name='wine_id')

for row, true_wine in zip(hit_user_meta.values, hit_wine_true):

    tmp_df = pd.DataFrame(itertools.repeat(row, len(wine_meta)), columns=hit_user_meta.columns)
    tmp_all = pd.concat([tmp_df, wine_meta], axis=1)
    tmp_input = {name:tmp_all[name] for name in feature_names}

    y_pred = model.predict(tmp_input, batch_size=BATCH_SIZE)
    y_pred = pd.Series(y_pred.T[0], name='y_pred')
    input_predict = pd.concat([wine_id, y_pred], axis=1)
    input_predict = input_predict.sort_values(by='y_pred', ascending=False)
    predict_top_k = input_predict.wine_id[:K].values

    if true_wine in predict_top_k:hit+=1

    i += 1

hit_rate = hit/len(hit_user_meta)

hit_rate



